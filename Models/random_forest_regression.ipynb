{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "oYOR8vrR-AfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVNH4G-d84XX"
      },
      "outputs": [],
      "source": [
        "# Random Forest Regression Model (Ensemble learning technique)\n",
        "\n",
        "# Import necessary libraries for data handling, visualization, and model evaluation\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "!pip install memory_profiler # Install memory profiler to track memory usage\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "from memory_profiler import memory_usage\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error,r2_score,mean_absolute_percentage_error,mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Dataset upload"
      ],
      "metadata": {
        "id": "oC7efWgF9-Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Dataset upload\n",
        "# Prompt the user to upload the dataset\n",
        "upload = files.upload()\n",
        "dataset = pd.read_csv('All_Div_BD.csv') # Load dataset into a DataFrame\n",
        "dataset.shape # Print the shape of the dataset\n",
        "dataset.head(5) # Display the first 5 rows for preview"
      ],
      "metadata": {
        "id": "sxqNo80A9-fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Exclude nighttime data (hours outside 7 AM - 7 PM)"
      ],
      "metadata": {
        "id": "MfgX4Tiy97PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Exclude nighttime data (hours outside 7 AM - 7 PM)\n",
        "# Filter rows with hour < 7 or hour > 19\n",
        "before_seven_am = dataset[dataset['hour']<7]\n",
        "after_seven_pm = dataset[dataset['hour']>19]\n",
        "before_seven_am.head(10)\n",
        "after_seven_pm.head(10)\n",
        "dataset = dataset.drop(before_seven_am.index, axis=0) # Drop early morning rows\n",
        "dataset = dataset.drop(after_seven_pm.index, axis=0) # Drop late evening rows"
      ],
      "metadata": {
        "id": "a3BNp7mF97bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Correlation matrix visualization"
      ],
      "metadata": {
        "id": "2fZh2pGt93mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Correlation matrix visualization\n",
        "# Analyze correlations between features\n",
        "dataset.corr()\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(dataset.corr(), annot=True)"
      ],
      "metadata": {
        "id": "q5u_2TyA937j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Define dependent (target) and independent (features) variables"
      ],
      "metadata": {
        "id": "mOLKh1V59yTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define dependent (target) and independent (features) variables\n",
        "# Exclude irrelevant columns from the feature set\n",
        "x = dataset.drop(['year','month','day','hour','wbgt'],axis=1)\n",
        "print(x.shape) # Print the shape of the independent variables\n",
        "colms = x.shape[1] # Extract the number of features (columns) in the independent variables\n",
        "colms # Output the number of features\n",
        "y = dataset['wbgt'] # Target variable: WBGT\n",
        "y.shape # Print the shape of the dependent variable"
      ],
      "metadata": {
        "id": "CPgMUMtv9yiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Train, test, and validation split"
      ],
      "metadata": {
        "id": "yrCUAuht9uhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train, test, and validation split\n",
        "# Split the dataset into training (80%) and test (20%) sets\n",
        "X_main,X_test,y_main,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n",
        "# Further split the training set into training (70%) and validation (10%) sets\n",
        "X_train,X_val,y_train,y_val = train_test_split(X_main,y_main,test_size=0.1,random_state=0)\n",
        "print(X_val.shape) # Print the shape of the validation set\n",
        "print(X_test.shape) # Print the shape of the test set\n",
        "val_rows = X_val.shape[0] # Store the number of rows in the validation set\n",
        "test_rows = X_test.shape[0] # Store the number of rows in the test set\n",
        "print(val_rows)\n",
        "print(test_rows)"
      ],
      "metadata": {
        "id": "qdae_pYt9utC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Model training with Random Forest"
      ],
      "metadata": {
        "id": "2OrsS33V9p3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Model training with Random Forest\n",
        "# Create a Random Forest Regressor with 50 decision trees and Out-of-Bag score\n",
        "reg_obj = RandomForestRegressor(n_estimators=50,oob_score=True)\n",
        "reg_obj.fit(X_train,y_train) # Fit the model on the training data\n",
        "print(\"OOB score: %f\"%reg_obj.oob_score_) # Print Out-of-Bag score"
      ],
      "metadata": {
        "id": "IK8lVmff9qFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Predictions for test and validation sets"
      ],
      "metadata": {
        "id": "f0vCskD99m0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Predictions for test and validation sets\n",
        "# Predict WBGT for the test set\n",
        "y_test_pred = reg_obj.predict(X_test)\n",
        "# Predict WBGT for the validation set\n",
        "y_val_pred = reg_obj.predict(X_val)"
      ],
      "metadata": {
        "id": "O79VovzO9nRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Performance and error metrics"
      ],
      "metadata": {
        "id": "bjbDy5AG9fpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Performance and error metrics\n",
        "# Test set\n",
        "r2_test = r2_score(y_test,y_test_pred)\n",
        "adjusted_r2_test = 1-((1-r2_test)*(test_rows-1)/(test_rows-1-colms))\n",
        "print(\"R2 score: %f\"%r2_test)\n",
        "print(\"Adjusted R2 score: %f\"%adjusted_r2_test)\n",
        "print(\"MAE: %f\"%mean_absolute_error(y_test,y_test_pred))\n",
        "print(\"MAPE: %f\"%mean_absolute_percentage_error(y_test,y_test_pred))\n",
        "print(\"MSE: %f\"%mean_squared_error(y_test,y_test_pred))\n",
        "print(\"RMSE: %f\"%np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
        "# Validation set\n",
        "r2_val = r2_score(y_val,y_val_pred)\n",
        "adjusted_r2_val = 1-((1-r2_val)*(val_rows-1)/(val_rows-1-colms))\n",
        "print(\"R2 score: %f\"%r2_val)\n",
        "print(\"Adjusted R2 score: %f\"%adjusted_r2_val)\n",
        "print(\"MAE: %f\"%mean_absolute_error(y_val,y_val_pred))\n",
        "print(\"MAPE: %f\"%mean_absolute_percentage_error(y_val,y_val_pred))\n",
        "print(\"MSE: %f\"%mean_squared_error(y_val,y_val_pred))\n",
        "print(\"RMSE: %f\"%np.sqrt(mean_squared_error(y_val,y_val_pred)))"
      ],
      "metadata": {
        "id": "E3pYI5LT9f4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Customize font for plots"
      ],
      "metadata": {
        "id": "ESN6IQvg9bvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Customize font for plots\n",
        "font_upload = files.upload() # Upload custom font file\n",
        "font_path = '/content/arial.ttf' # Path to the uploaded font file\n",
        "fm.fontManager.addfont(font_path) # Add font to Matplotlib\n",
        "# Apply font customization globally\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = ['Arial']\n",
        "plt.rcParams['font.size'] = 15"
      ],
      "metadata": {
        "id": "IbqLz1Hj9b7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Visualize the importance of each feature"
      ],
      "metadata": {
        "id": "45Eaprqa9Yls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Visualize the importance of each feature\n",
        "importances = reg_obj.feature_importances_ # Feature importance values\n",
        "importances\n",
        "reg_obj.feature_importances_.shape\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.bar(x.columns, importances) # Bar chart of feature importance\n",
        "plt.ylabel('Feature importance')\n",
        "# Annotate each bar with its value\n",
        "for index, value in enumerate(importances):\n",
        "    plt.text(index, value+0.02, str(value), ha='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1DVk5uje9Y0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11: Scatter plot of Actual vs Predicted values"
      ],
      "metadata": {
        "id": "XmBChS4P9UBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Scatter plot of Actual vs Predicted values\n",
        "plt.scatter(y_test, y_test_pred, alpha=1)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test_pred), max(y_test_pred)], color='red', linewidth=2)\n",
        "plt.legend([\"Predicted\",\"Actual\"], loc=\"upper left\")\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('WBGT')"
      ],
      "metadata": {
        "id": "stclxX6N9UUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 12: Inference time and memory usage"
      ],
      "metadata": {
        "id": "pQ9BxEre9MCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Inference time and memory usage\n",
        "# Test set\n",
        "test_initial_time = time.time()\n",
        "print(test_initial_time)\n",
        "X_test_temp_index = random.randrange(0,len(X_test)) # Choose a random row\n",
        "X_test_temp = X_test.iloc[X_test_temp_index].values.reshape(1,-1) # Reshape for prediction\n",
        "y_pred = reg_obj.predict(X_test_temp) # Perform prediction\n",
        "test_final_time = time.time()\n",
        "print(test_final_time)\n",
        "test_inference_time = test_final_time - test_initial_time # Calculate inference time\n",
        "print('Test set inference time:', test_inference_time)\n",
        "# Calculate average memory usage for prediction\n",
        "def y_pred_func():\n",
        "  return reg_obj.predict(X_test_temp)\n",
        "\n",
        "test_mem_usage = memory_usage(y_pred_func) # Track memory usage\n",
        "test_avg_mem_usage = np.mean(test_mem_usage) # Calculate average memory usage\n",
        "print('Test set avg. memory usage:', test_avg_mem_usage)\n",
        "# Validation set\n",
        "val_initial_time = time.time()\n",
        "print(val_initial_time)\n",
        "X_val_temp_index = random.randrange(0,len(X_val)) # Choose a random row\n",
        "X_val_temp = X_val.iloc[X_val_temp_index].values.reshape(1,-1) # Reshape for prediction\n",
        "y_val_pred = reg_obj.predict(X_val_temp) # Perform prediction\n",
        "val_final_time = time.time()\n",
        "print(val_final_time)\n",
        "val_inference_time = val_final_time - val_initial_time # Calculate inference time\n",
        "print('Validation set inference time:', val_inference_time)\n",
        "# Calculate average memory usage for prediction\n",
        "def y_val_pred_func():\n",
        "  return reg_obj.predict(X_val_temp)\n",
        "\n",
        "val_mem_usage = memory_usage(y_val_pred_func) # Track memory usage\n",
        "val_avg_mem_usage = np.mean(val_mem_usage) # Calculate average memory usage\n",
        "print('Validation set avg memory usage:', val_avg_mem_usage)"
      ],
      "metadata": {
        "id": "SfiuLZv69MQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 13: K-Fold cross-validation (K=10)"
      ],
      "metadata": {
        "id": "rRap_EF-9Agr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 13: K-Fold cross-validation (K=10)\n",
        "k_fold = KFold(n_splits=10) # Create K-Fold cross-validator\n",
        "k_fold\n",
        "cv_score = cross_val_score(reg_obj,x,y, cv=k_fold) # Perform cross-validation\n",
        "print(\"Cross-validation scores:\", cv_score) # Print cross-validation scores"
      ],
      "metadata": {
        "id": "-_Cb5S3L9Cak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}